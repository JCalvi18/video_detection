# -*- coding: utf-8 -*-
"""CenterNet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yr2qeJBSEEDpdh-5mgdctMXZP2_OqCDi
"""

from IPython.display import clear_output

# Commented out IPython magic to ensure Python compatibility.
#Clone CenterNet
!git clone https://github.com/xingyizhou/CenterNet
!pip install progress
#Build nms
# %cd /content/CenterNet/src/lib/external/
!python setup.py build_ext --inplace
#Build and install DCNv2 from the original repo
# %cd ../models/networks/
!rm -rf DCNv2
!git clone https://github.com/CharlesShang/DCNv2
# %cd DCNv2/src/cuda/
!sed -i '11s/.*/THCState *state = at::globalContext().lazyInitCUDA();/' dcn_v2_cuda.cu
# %cd ../..
!python setup.py build develop
clear_output()

# Commented out IPython magic to ensure Python compatibility.
#Download pretrained model
# %cd /content/CenterNet/src
!gdown --id 1pl_-ael8wERdUREEnaIfqOV_VF2bEVRT
!mv ctdet_coco_dla_2x.pth ../models/
#!gdown --id 1PO1Ax_GDtjiemEmDVD7oPWwqQkUu28PI
#!mv multi_pose_dla_3x.pth ../models/
#!gdown --id 1znsM6E-aVTkATreDuUVxoU0ajL1az8rz
#!mv ddd_3dop.pth ../models/
#!gdown --id 15XuJxTxCBnA8O37M_ghjppnWmVnjC0Hp
#!mv ddd_sub.pth ../models/
clear_output()

## Prepare data using youtube video
#Install YouTube DL
!sudo apt-get install curl -y
!curl -L https://yt-dl.org/latest/youtube-dl -o /usr/bin/youtube-dl
!sudo chmod 755 /usr/bin/youtube-dl
!youtube-dl -f 137 https://www.youtube.com/watch?v=tHRLX8jRjq8 --output 'skyfall.%(ext)s'
!ffmpeg -i skyfall.mp4 -ss 00:00:00.00 -t 00:00:20.00 -c copy ../images/cskyfall.mp4
#Human pose using taichi video
!youtube-dl -f 135 'https://youtu.be/olTSwTofrDM' -o 'tai.%(ext)s'
!ffmpeg -i tai.mp4 -ss 00:00:15.00 -t 00:00:20.00 -c copy ../images/ctai.mp4
!rm tai.mp4
clear_output()

# Commented out IPython magic to ensure Python compatibility.
## Prepare data using mini market video
# %cd /content
!git clone https://github.com/JCalvi18/video_detection.git
!mv video_detection/centernet/*py CenterNet/src/
#Add faces folder
!rm -rf video_detection/
#Add video from the drive folder to the resources 1/2
from google.colab import drive
drive.mount('/content/drive')
#Add video from the drive folder to the resources 2/2
!mkdir CenterNet/exp/mini
!cp '/content/drive/My Drive/lestartap/FaceRecognition/variete.mp4' 'CenterNet/exp/mini/'
clear_output()

#Correct line 13 of debugger.py in order to allow matplotlib import
!sed -i '13s/if not self.ipynb/if self.ipynb/' lib/utils/debugger.py

from IPython.display import clear_output

# Commented out IPython magic to ensure Python compatibility.
###### START HERE ###################
# %cd /content/CenterNet/src
import sys
import os
import time
CENTERNET_PATH_LIB = os.path.abspath('.')+'/lib'
if CENTERNET_PATH_LIB not in sys.path: #ADD CenterNet to the python path
  sys.path.append(CENTERNET_PATH_LIB)
#sys.path.remove(CENTERNET_PATH_LIB)

# Commented out IPython magic to ensure Python compatibility.
#Run the demo to test if the configuration is correct
# %run demo.py ctdet --demo ../images/17790319373_bd19b24cfc_k.jpg --load_model ../models/ctdet_coco_dla_2x.pth --debug 3

# Commented out IPython magic to ensure Python compatibility.
# %run demo.py ddd --demo ../images/cjam.mp4 --load_model ../models/ddd_3dop.pth --debug 3

start = time.time()
import numpy as np
import cv2
import matplotlib.pyplot as plt
from matplotlib import animation
from IPython.display import HTML
from detectors.detector_factory import detector_factory
from opts import opts
from utils.debugger import Debugger
from datetime import datetime

ptrain_dir={'ddd':'../models/ddd_sub.pth','ctdet':'../models/ctdet_coco_dla_2x.pth','multi_pose':'../models/multi_pose_dla_3x.pth'}
TASK = 'multi_pose' # or 'multi_pose' for human pose estimation
MODEL_PATH = ptrain_dir[TASK]


opt = opts().init('{} --load_model {}'.format(TASK, MODEL_PATH).split(' '))
detector = detector_factory[opt.task](opt)

image_ext = ['jpg', 'jpeg', 'png', 'webp']
video_ext = ['mp4', 'mov', 'avi', 'mkv']
image_in = ['img','cam','video']

#in_file = '../images/cskyfall.mp4'
in_file = '../images/ctai.mp4'
#in_file = '../images/17790319373_bd19b24cfc_k.jpg'
#in_file = 'rtsp://adm:12345@181.188.171.31:554/cam/realmonitor?channel=6&subtype=0'
#out_file = '../exp/skyfall.mp4'
out_file = '../exp/taipose.mp4'
#out_file = '../exp/'+TASK+'_'+datetime.now().strftime('%d_%H-%M')+'.'+video_ext[0]

total_frames = 200

#prepare Input data
if isinstance(in_file,np.ndarray):
  ext = 'jpg'
else:
  ext = in_file[in_file.rfind('.') + 1:].lower()

if ext in image_ext:
  frames=[in_file]
  total_frames=1
else:
  cap = cv2.VideoCapture(in_file)  # Create a VideoCapture object
  if ext in video_ext:
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
  frame_w, frame_h = int(cap.get(3)), int(cap.get(4)) # Convert resolutions from float to integer.
  if (cap.isOpened() == False): # Check if camera opened successfully
    print("Unable to read camera feed")
  frames=[]
  for _ in range(total_frames):
    ret, frame = cap.read()
    if ret:
      frames.append(frame)
  cap.release()

#Process input data
results=[]
for frame in frames:
  ret = detector.run(frame)['results']
  results.append(ret)

debugger = Debugger(dataset=opt.dataset, ipynb=True,theme=opt.debugger_theme)
for i, (img,res) in enumerate(zip(frames,results)):
  if TASK == 'ctdet': #Add bounding box
    frame_id=TASK+'_frame'+str(i)
    debugger.add_img(img, img_id=frame_id)
    for j in range(1, detector.num_classes + 1):
      for bbox in res[j]:
        if bbox[4] > opt.vis_thresh:
          debugger.add_coco_bbox(bbox[:4], j - 1, bbox[4], img_id=frame_id)
  elif TASK == 'ddd': #Add 3d box
    frame_id=TASK+'_frame3D'+str(i)
    debugger.add_3d_detection(img, res, detector.this_calib,center_thresh=opt.vis_thresh, img_id=frame_id)
    frame_id=TASK+'_frameBird'+str(i)
    debugger.add_bird_view(res, center_thresh=opt.vis_thresh, img_id=frame_id)
  else: #Human pose
    frame_id=TASK+'_frame'+str(i)
    debugger.add_img(img, img_id=frame_id)
    for bbox in res[1]:
      if bbox[4] > opt.vis_thresh:
        debugger.add_coco_bbox(bbox[:4], 0, bbox[4], img_id=frame_id)
        debugger.add_coco_hp(bbox[5:39], img_id=frame_id)

#prepare Output data
if out_file is None:
  ext = None
else:
  ext = out_file[out_file.rfind('.') + 1:].lower()

if TASK == 'ctdet':
  out_imgs = [v for v in debugger.imgs.values()]
elif TASK == 'ddd':
  out_imgs = [v for k,v in debugger.imgs.items() if '3D' in k]
else:
  out_imgs = [v for k,v in debugger.imgs.items() if TASK in k]

if ext is None:
  plt.axis('off')
  plt.imshow(cv2.cvtColor(out_imgs[0], cv2.COLOR_BGR2RGB))
elif ext in image_ext:
  cv2.imwrite(out_file,out_imgs[0])
elif ext in video_ext:
  # Define the codec and create VideoWriter object.The output will be stored as a .mp4 file.
  out = cv2.VideoWriter(out_file, cv2.VideoWriter_fourcc('m','p','4','v'), 15, (frame_w, frame_h))
  for v in out_imgs:
    out.write(v)
  out.release()
